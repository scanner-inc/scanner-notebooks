{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92bcb11-06b1-4270-ade8-d7e2e2e3d772",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "This is an example where the **Isolation Forest** algorithm is used to detect anomalies in CloudTrail logs.\n",
    "\n",
    "## How It Works\n",
    "Isolation Forest operates by constructing a forest of random trees (i.e., an ensemble of decision trees). Each tree is built by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The primary idea is that, in a randomly partitioned tree, anomalies will require fewer splits to isolate them compared to normal points, which are more similar to each other and thus require more splits.\n",
    "\n",
    "1. **Random Splitting**: Each tree is built by randomly selecting a feature and a split value. This random partitioning helps in isolating the data points quickly.\n",
    "1. **Path Length**: The number of splits required to isolate a data point is known as the path length. Anomalies, being few and distinct, are expected to have shorter path lengths in the trees.\n",
    "1. **Scoring**: The anomaly score is calculated based on the path length. Points with shorter average path lengths across the trees in the forest are considered anomalies.\n",
    "\n",
    "## Advantages\n",
    "- **Efficiency**: Isolation Forest is highly efficient and can handle large datasets with low memory requirements.\n",
    "- **Performance**: It is particularly effective for detecting anomalies in high-dimensional datasets.\n",
    "- **Interpretability**: The concept of path length and isolation provides a straightforward interpretation of why a point is considered an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b4ffa-79dc-404f-b0f1-3263e2fc7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn matplotlib pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f78fb5-2f56-46e5-b3f3-7b7dba4a16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install https://scanner-dev-public.s3.us-west-2.amazonaws.com/sdks/python/scanner_client-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e741215-cec6-4c03-9dc0-df969bdd22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scanner_client import Scanner\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191fd2f-c697-4c88-afbd-dd5ea49a5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_data_frame(results):\n",
    "    rows = [row.columns.to_dict() for row in results.rows]\n",
    "    column_tags = results.column_tags.to_dict()\n",
    "    if len(column_tags) > 0:\n",
    "        # If this is a table, use the column ordering in the data frame\n",
    "        return pd.DataFrame(data=rows, columns=results.column_ordering)\n",
    "    else:\n",
    "        # Otherwise, this is a list of log events, so use pandas JSON\n",
    "        # normalization to set the table columns to the union of all keys.\n",
    "        return pd.json_normalize(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf45cc-ad50-4172-8fdd-2caef788bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scanner(\n",
    "    api_url=os.environ[\"SCANNER_API_URL\"],\n",
    "    api_key=os.environ[\"SCANNER_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8738d9d-ef83-4765-829a-7d5d78d265e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now(tz=timezone.utc)\n",
    "start_time = end_time - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcf7b5-a195-45f1-bec7-b27f0f1c7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = scanner.query.blocking_query(\n",
    "    start_time=start_time.isoformat(),\n",
    "    end_time=end_time.isoformat(),\n",
    "    query_text=\"\"\"\n",
    "        %ingest.source_type: \"aws:cloudtrail\"\n",
    "        userIdentity.type: IAMUser\n",
    "    \"\"\"\n",
    ")\n",
    "df = convert_results_to_data_frame(response.results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc055f9-25da-47c9-8915-195fbd3c5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'eventSource', 'eventName', 'userIdentity.arn', 'sourceIPAddress', 'eventTime', \n",
    "    'awsRegion', 'eventHour', 'eventDate',\n",
    "]\n",
    "\n",
    "df['eventHour'] = pd.to_datetime(df['eventTime']).dt.hour\n",
    "df['eventDate'] = pd.to_datetime(df['eventTime']).dt.date\n",
    "\n",
    "df = df[features]\n",
    "\n",
    "# Encode categorical features. i.e. turn a string enum into a number\n",
    "df['encEventDate'] = pd.factorize(df['eventDate'])[0]\n",
    "df['encEventSource'] = pd.factorize(df['eventSource'])[0]\n",
    "df['encEventName'] = pd.factorize(df['eventName'])[0]\n",
    "df['encSourceIPAddress'] = pd.factorize(df['sourceIPAddress'])[0]\n",
    "df['encUserIdentityArn'] = pd.factorize(df['userIdentity.arn'])[0]\n",
    "df['encAwsRegion'] = pd.factorize(df['awsRegion'])[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e9d25-77f3-45c1-abd8-f039b97c3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = [\n",
    "    'eventHour', 'encEventDate', 'encEventSource', 'encEventName', 'encSourceIPAddress', 'encUserIdentityArn', 'encAwsRegion',\n",
    "]\n",
    "model_df = df[model_features]\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d263cd0-75d2-43d1-83eb-caf8f82ddf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(model_df)\n",
    "\n",
    "# Train Isolation Forest model\n",
    "clf = IsolationForest(contamination=0.01, random_state=42)\n",
    "df['anomaly'] = clf.fit_predict(X)\n",
    "\n",
    "# Interpret results\n",
    "df['anomaly'] = df['anomaly'].map({1: 0, -1: 1})  # Convert to 0 (normal) and 1 (anomaly)\n",
    "\n",
    "# Anomalies\n",
    "anomalies = df[df['anomaly'] == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbab72-d8f8-4d13-a48b-de4a7a1a43c4",
   "metadata": {},
   "source": [
    "# Anomalies Detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6a695-4a48-46a4-b80f-25eebf7eea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomalies found:\")\n",
    "print(len(anomalies))\n",
    "\n",
    "anomalies[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe681a-c7c6-4999-8ba6-e09a3f78bf89",
   "metadata": {},
   "source": [
    "# Anomalies by Event Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a0c04-4179-41b5-ac9a-dbb96e0e2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming anomalies is your DataFrame with the anomaly data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=anomalies, x='eventName')\n",
    "plt.title('Anomalies by Event Name')\n",
    "plt.xlabel('Event Name')\n",
    "plt.ylabel('Count of Anomalies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38556246-5240-4caf-a838-e16aebc6c7d8",
   "metadata": {},
   "source": [
    "# Heatmap of Anomalies by Hour and Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17097ea5-340c-4980-955c-c242e1b9d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pivot table for the heatmap\n",
    "heatmap_data = anomalies.pivot_table(index='eventHour', columns='eventDate', aggfunc='size', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d')\n",
    "plt.title('Heatmap of Anomalies by Hour and Date')\n",
    "plt.xlabel('Event Date')\n",
    "plt.ylabel('Event Hour')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
